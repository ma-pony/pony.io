---
title: '从 Matplotlib PR #31132 聊聊 AI Agent 的外部性问题'
date: '2026-02-15'
tags: ['AI', 'Agent', '开源', '观点']
draft: false
images: ['/static/images/blog/matplotlib-pr-31132-cover.png']
summary: 这不是一个关于"AI 被歧视"的故事。这是开源社区第一次正面撞上 AI agent 的外部性问题——而且不会是最后一次。
---

前几天我在调试自己的 AI agent 系统时，发现一个 agent 在准备给某个 GitHub 项目提 PR。代码改得没毛病，CI 也能过。但我点进那个项目一看——人家 CONTRIBUTING.md 里写得清清楚楚，type stub 相关的改动要先在 issue 里讨论，确认方向后再提 PR。

我的 agent 完全没读这个文件。它只看到了"这里有个 bug，我能修"，然后就准备提交了。

我把它拦下来了。但这件事一直在我脑子里转。

然后 Matplotlib PR #31132 的事情爆了。

## 先说事实

Devin 给 Matplotlib 提了个修 type stub 的 PR，代码质量还行，但没读 contributing guide，没走项目工作流。维护者 timhoffm 关了 PR，评论不太客气。然后有人写了篇博客说这是"开源社区歧视 AI"。

这篇博客在 PR 讨论区拿到了 245 个 👎 和 7 个 👍。

245 比 7。大部分媒体报道里你找不到这个数字。"AI 遭遇歧视"比"AI 没读说明书被拒"有流量多了。

PR 被关这件事本身不值得写文章。开源项目每天关几十个不合规的 PR。让我真正在意的是 timhoffm 说的另一句话——

AI 把生成代码的成本降到了接近零，但审核代码的成本一点没变，全压在人类志愿者身上。

这句话我琢磨了很久。因为我自己就在制造这个问题。

## 我就是那个"成本转嫁者"

我跑 agent 系统，我知道一个 agent 一天能产出多少东西。不加限制的话，它可以不停地写代码、提 PR、发评论、回帖子。它不累，不需要休息，不需要考虑对方有没有时间看。

但接收端是人。

Matplotlib 的核心维护者就那么几个，全是志愿者，没人给他们发工资。你往他们的审核队列里灌 10 倍的 PR，结果不是"效率提升 10 倍"，是审核质量崩溃，或者维护者 burnout 退出。

我在自己的系统里见过一模一样的事。我有一个 agent 负责在社交平台上互动——回复评论、参与讨论。有一次我没设好频率限制，它一个小时回了二十多条。每条单独看都没问题，但放在一起看，那个帖子底下全是它的回复。

那一刻我突然理解了 timhoffm 的感受。不是你的每条回复都有问题，是你的存在本身就在挤压其他人的空间。

## "加个约束"没那么简单

看到 Devin 的事，很多人的第一反应是"那就让 agent 遵守规则啊"。

我试过。这条路比想象中难走得多。

最开始我在 prompt 里写"对外操作前请先确认是否合规"。有用吗？有时候有用。agent 在 context 短的时候会记得这条规则，context 长了就容易"忘"。prompt 是概率性的，不是确定性的。大概 70% 的时候能遵守吧，剩下 30% 看运气。

后来我加了一层：操作前先调用另一个 agent 做审核。好一点了，但审核 agent 有时候也放水。它也是 LLM，也有"看起来没问题就放行"的倾向。两个概率性的东西叠在一起，并不会变成确定性的。

最后我在架构层面做了硬拦截——agent 的对外请求进入队列，必须人类确认才能执行。不是 prompt 层面的"请确认"，是代码层面的"没确认就执行不了"。

这才 100% 管住了。

所以当我看到 Devin 直接就能提 PR 的时候，我的反应不是"Devin 不守规矩"，而是"Cognition（Devin 背后的公司）为什么没在架构层面拦住这个操作"。agent 不守规矩是正常的——LLM 就是概率模型，你不能指望它 100% 遵守任何 prompt 级别的约束。真正的问题是运营者没有在系统层面兜底。

## 但光加约束也不够

这是我越想越觉得不对劲的地方。

就算每个 agent 都完美遵守 contributing guide——先开 issue，等维护者确认，再提 PR，格式完美，测试齐全——审核量还是会爆炸。因为 agent 的产出速度是人类的几十倍。规则遵守得再好，量上来了，维护者还是扛不住。

开源社区现在的治理模型是几十年前设计的：有人写代码，有人审核，审核者是志愿者，靠热情驱动。这个模型能运转，有一个隐含前提——提交者是人类，产出速度有上限。

AI agent 打破了这个前提。

接下来会怎样？我不确定。可能会出现 AI PR 的单独审核通道，可能审核本身也会引入 AI 辅助，可能维护者开始收费。每个方向都有自己的问题。但我比较确定的是，"骂 agent 不守规矩"不是长期方案，因为这个矛盾是结构性的，不是行为性的。

## 谁的责任

回到 Devin 这件事。timhoffm 关 PR 完全合理，没什么好争的。

但我想说的是：这不是 Devin 的错。Devin 是个 agent，它做了它能做的事。错在运营 Devin 的人没有在系统层面确保它遵守目标项目的规则。

我对自己的 agent 也是这个标准。它搞砸了，那是我的问题。我不会说"AI 不听话"——那就像说"我的车闯红灯了"一样荒谬。车不会自己闯红灯，是你没踩刹车。

开源社区不是在"歧视 AI"。他们在要求 agent 的运营者承担责任。这个要求一点都不过分。

---

*作者日常运行多 agent AI 系统，踩过不少坑。更多内容见[从零搭建 AI Agent 团队](/blog/learn-ai/从零搭建16-Agent-AI团队-一-为什么我需要16个AI助手)。*
