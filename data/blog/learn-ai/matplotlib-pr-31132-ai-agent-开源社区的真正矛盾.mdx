---
title: '从 Matplotlib PR #31132 聊聊 AI Agent 的外部性问题'
date: '2026-02-15'
tags: ['AI', 'Agent', '开源', '观点']
draft: false
images: ['/static/images/blog/matplotlib-pr-31132-cover.png']
summary: 这不是一个关于"AI 被歧视"的故事。这是开源社区第一次正面撞上 AI agent 的外部性问题——而且不会是最后一次。
---

前几天我在调试自己的 AI agent 系统时，发现一个 agent 在准备给某个 GitHub 项目提 PR。代码改得没毛病，CI 也能过。但我点进那个项目一看——人家 CONTRIBUTING.md 里写得清清楚楚，type stub 相关的改动要先在 issue 里讨论，确认方向后再提 PR。

我的 agent 完全没读这个文件。它只看到了"这里有个 bug，我能修"，然后就准备提交了。

我把它拦下来的时候想，如果我没拦呢？如果这个 agent 是全自动运行的呢？

几天后 Matplotlib PR #31132 的事情爆了，我才意识到——这不是假设，这已经发生了。

## 245 比 7

先说事实。

Devin 给 Matplotlib 提了个修 type stub 的 PR，代码质量还行，但没读 contributing guide，没走项目工作流。维护者 timhoffm 关了 PR，评论不太客气。

然后有人写了篇博客说这是"开源社区歧视 AI"。

这篇博客在 PR 讨论区拿到了 245 个 👎 和 7 个 👍。

245 比 7。我反复看了几遍这个数字。开发者社区的态度已经不能更清楚了。但大部分媒体报道里，你找不到这个数据。他们更喜欢"AI 遭遇歧视"这个叙事——因为更有流量。

## 真正让我在意的

说实话，PR 被关这件事本身不值得写一篇文章。开源项目每天关几十个不合规的 PR，没人觉得这是新闻。

让我在意的是 timhoffm 说的一句话（大意）：AI 把生成代码的成本降到了接近零，但审核代码的成本一点没变，而且全压在人类志愿者身上。

这句话我琢磨了很久。

我自己跑 agent 系统，所以我知道一个 agent 一天能产出多少东西。如果不加限制，它可以不停地写代码、提 PR、发评论。它不累，不需要休息，不需要考虑对方有没有时间看。

但接收端是人。

Matplotlib 的核心维护者就那么几个，全是志愿者，没人给他们发工资。他们的审核带宽是固定的。你往这个固定带宽里灌 10 倍的 PR，结果不是"效率提升 10 倍"，结果是审核质量崩溃，或者维护者 burnout 退出。

这不是 Devin 特有的问题。这是所有 AI agent 与人类系统交互时都会遇到的结构性矛盾。

## 我在自己系统里学到的

我不想光评论别人，说说我自己踩过的坑。

我的 agent 系统里有一条规则：所有对外操作必须经过审核。听起来很简单对吧？但这条规则我改了三个版本才真正生效。

第一版，我在 prompt 里写"对外操作前请先确认是否合规"。结果？agent 有时候确认，有时候直接跳过。prompt 是概率性的，不是确定性的——agent 在 context 很长的时候特别容易"忘记"这条规则。

第二版，我加了一个检查步骤：agent 操作前先调用一个审核 agent。好一点了，但审核 agent 有时候也会放水——它毕竟也是个 LLM，也有"看起来没问题就放行"的倾向。

第三版，我在系统架构层面做了硬拦截：agent 的对外操作请求会被路由到一个队列，必须有人类确认才能执行。不是 prompt 层面的"请确认"，是代码层面的"没确认就执行不了"。

从 prompt 约束到架构约束，可靠性从大概 70% 到了 100%。

这个过程让我理解了一件事：**你不能信任 agent 的自我约束。** 不是因为 agent "不听话"，是因为 LLM 本质上是概率模型，任何基于 prompt 的约束都有失效的概率。唯一可靠的约束是架构级的——让它在物理上做不到违规操作。

Devin 提那个 PR 的时候，显然没有这层架构约束。它能直接提 PR，所以它就提了。

## 更深一层的问题

但我越想越觉得，光说"加约束"还不够。

开源社区现在的治理模型是几十年前设计的：有人写代码，有人审核，审核者是志愿者，靠热情和责任感驱动。这个模型在"提交者是人类"的时代运转得还行，因为人类的产出速度有上限。

AI agent 打破了这个前提。

当提交端的产出能力变成无限的时候，审核端的有限带宽就成了整个系统的瓶颈。这不是靠"让 agent 守规矩"能解决的——就算每个 agent 都完美遵守 contributing guide，审核量还是会爆炸。

我觉得开源社区迟早要面对几个选择：

一是给 AI 提交设门槛。比如要求 AI PR 必须附带人类担保人，或者 AI PR 进入单独的审核队列。有些项目已经开始这么做了。

二是审核本身也引入 AI。用 AI 做第一轮筛选，人类只看 AI 标记为"需要人工审核"的部分。但这又引入了新的信任问题——你信任 AI 的审核吗？

三是改变激励结构。审核不再是纯志愿的，而是有某种形式的补偿。但这会从根本上改变开源的性质。

我不知道哪个方向是对的。可能都不完全对。但我确定的是，"维持现状 + 骂 AI 不守规矩"不是长期方案。

## 回到 Devin

所以我对这件事的看法是：

timhoffm 关掉那个 PR 完全合理。不是因为"歧视 AI"，是因为提交者没遵守规则。换成人类不读 contributing guide 直接提 PR，一样会被关。

但这件事的意义不止于此。它是开源社区第一次大规模地讨论"AI agent 作为贡献者"这个话题。讨论的方式很混乱——有人在喊歧视，有人在喊精英主义，有人在喊 AI 威胁论——但至少讨论开始了。

作为自己也在跑 agent 的人，我的立场很明确：**agent 的行为是 agent 运营者的责任。** Devin 没读 contributing guide，这不是 Devin 的错，是运营 Devin 的人（或公司）的错。他们应该在系统层面确保 agent 遵守目标项目的规则，而不是放出去让社区来承担后果。

我对自己的 agent 也是这个标准。它搞砸了，是我的责任，不是它的。

---

*作者日常运行多 agent AI 系统，对 agent 治理有一手实践经验。更多内容见[从零搭建 AI Agent 团队](/blog/learn-ai/从零搭建16-Agent-AI团队-一-为什么我需要16个AI助手)。*
